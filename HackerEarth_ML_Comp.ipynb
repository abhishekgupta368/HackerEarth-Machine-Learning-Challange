{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HackerEarth ML Comp",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-QyFo0AY16S",
        "colab_type": "code",
        "outputId": "4be8e5c0-46fa-4a52-98e8-099429b944dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "! unzip files"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  files.zip\n",
            "   creating: dataset/\n",
            "  inflating: dataset/train.csv       \n",
            "  inflating: dataset/test.csv        \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zLhmuKdIiF0",
        "colab_type": "code",
        "outputId": "69a463c2-34db-417f-c860-d8aa5f58d527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdDFJuILIbwx",
        "colab_type": "code",
        "outputId": "2f74d9bb-ef0c-439e-a43a-9d6deecd7292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('words')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI4XIbiZONtq",
        "colab_type": "code",
        "outputId": "7b3db74a-c7e0-459b-e665-ae6d3b14573d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993193 sha256=ecf6db77f61642020691c9e4c14cf1bd590d4f1aef71015a678e63613b384302\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJe-lnwJLeaI",
        "colab_type": "code",
        "outputId": "598134e1-e7c7-4da3-81e6-9e942242c13c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "! pip install googletrans"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/f0/a22d41d3846d1f46a4f20086141e0428ccc9c6d644aacbfd30990cf46886/googletrans-2.4.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletrans) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->googletrans) (1.24.3)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-2.4.0-cp36-none-any.whl size=15777 sha256=3a4cf385bf622fa96320f7416ced079f1a35b4e6f4bee4e769a3794491848de7\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/d6/e7/a8efd5f2427d5eb258070048718fa56ee5ac57fd6f53505f95\n",
            "Successfully built googletrans\n",
            "Installing collected packages: googletrans\n",
            "Successfully installed googletrans-2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWuE9pAEacwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from googletrans import Translator\n",
        "from langdetect import detect \n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.externals import joblib\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvFwz0itcjV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PreprocessData():\n",
        "    def __init__(self,csv_file):\n",
        "        self.dataset = pd.read_csv(csv_file)\n",
        "        self.convert_baseform = PorterStemmer()\n",
        "        self.english_word = words = set(nltk.corpus.words.words())\n",
        "\n",
        "    def preprocess_string(self,data):\n",
        "        try:\n",
        "            data = data.lower()\n",
        "            tweet = re.sub(\"i’m\" ,'i am',data)\n",
        "            tweet = re.sub(\"doesn’t\",'does not',tweet)\n",
        "            tweet = re.sub(\"don’t\",'do not',tweet)\n",
        "            tweet = re.sub(\"mother’s\",'mothers',tweet)\n",
        "            tweet = re.sub(\"it’s\",'it is',tweet)\n",
        "            tweet = re.sub(\"can’t\",'can not',tweet)\n",
        "            tweet = re.sub(\"mum’s\",'mums',tweet)\n",
        "            brk_str = tweet.split(' ')\n",
        "            filter_hashtag = ' '.join(filter(lambda x:x[0]!='#',brk_str))\n",
        "            filter_at_the_rate = ' '.join(filter(lambda x:x[0]!='@',filter_hashtag.split(' ')))\n",
        "            remove_twitter_link = re.sub(r'(\\s)pic.twitter.com/\\w+','',filter_at_the_rate)\n",
        "\n",
        "            remove_broken_url=\"\"\n",
        "            if(remove_twitter_link.find(\"https\")):\n",
        "                remove_broken_url  = remove_twitter_link.split('https')[0].strip(' ')\n",
        "            elif(remove_twitter_link.find(\"http\")):\n",
        "                remove_broken_url  = remove_twitter_link.split('http')[0].strip(' ')\n",
        "\n",
        "            remove_puncations = \"\"\n",
        "            if(len(remove_broken_url)!=0):\n",
        "                remove_puncations = re.sub(r'[^\\w\\s]','',remove_broken_url)\n",
        "            else:\n",
        "                remove_puncations = re.sub(r'[^\\w\\s]','',remove_twitter_link)\n",
        "            \n",
        "            text_tokens = word_tokenize(remove_puncations)\n",
        "            remove_stopwords = [word for word in text_tokens if not word in stopwords.words('english')]\n",
        "            word_base_form = [self.convert_baseform.stem(word) for word in remove_stopwords]\n",
        "            \n",
        "            final_preprocessed_string =  ' '.join(word_base_form)\n",
        "            final_preprocessed_string = \" \".join(w for w in word_tokenize(final_preprocessed_string) if w is not w.isalpha())\n",
        "            return final_preprocessed_string\n",
        "\n",
        "        except:\n",
        "            print(\"Error in format\")\n",
        "            return data.lower()\n",
        "\n",
        "    def clean_dataset(self):\n",
        "        self.dataset['original_text'] = self.dataset['original_text'].apply(self.preprocess_string)\n",
        "\n",
        "    def getDataset(self):\n",
        "        return self.dataset\n",
        "    \n",
        "    def save_preprocessed_data_set(self):\n",
        "        self.dataset.to_csv('cleaned_comment.csv',index = False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jggne1xVgmyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentModel():\n",
        "    def __init__(self,dataset,testdata):\n",
        "        self.test_dataset = testdata\n",
        "\n",
        "        self.dataset = pd.read_csv(dataset)\n",
        "        self.dataset['sentiment_class'] = self.dataset['sentiment_class'].apply(self.convert_neg_to_pos)\n",
        "        self.text = self.dataset['original_text'].values\n",
        "        self.label = self.dataset['sentiment_class'].values\n",
        "        self.x_train,self.y_train = None,None\n",
        "        self.x_test,self.y_test = None,None\n",
        "        self.vectorizer = TfidfVectorizer()\n",
        "        self.mlModels = {'GaussianNB': GaussianNB,\n",
        "                         'MultinomialNB': MultinomialNB,\n",
        "                         'SVC': SVC,\n",
        "                         'KNeighborsClassifier': KNeighborsClassifier,\n",
        "                         'DecisionTreeClassifier': DecisionTreeClassifier,\n",
        "                         'RandomForestClassifier': RandomForestClassifier}\n",
        "        self.model = MultinomialNB()\n",
        "\n",
        "    def convert_neg_to_pos(self,n):\n",
        "        if(n== -1):\n",
        "            return 2\n",
        "        return n\n",
        "    \n",
        "    def convert_pos_to_neg(self,n):\n",
        "        if(n==2):\n",
        "            return -1\n",
        "        return n\n",
        "\n",
        "    def split_data(self):\n",
        "        self.x_train,self.x_test,self.y_train,self.y_test = train_test_split(self.text,self.label, test_size =0.1, random_state=42)\n",
        "    \n",
        "    def vectorize_data(self):\n",
        "        self.split_data()\n",
        "        self.x_train = self.vectorizer.fit_transform(self.x_train)\n",
        "        self.x_test = self.vectorizer.transform(self.x_test)\n",
        "    \n",
        "    def train_model(self):\n",
        "        self.model.fit(self.x_train.toarray(), self.y_train)\n",
        "\n",
        "    def test_accuratcy(self):\n",
        "        y_true, y_pred = self.y_test, self.model.predict(self.x_test.toarray())\n",
        "        joblib.dump(self.model, 'model.pkl') \n",
        "        print(accuracy_score(y_true,y_pred))\n",
        "    \n",
        "    def load_model(self):\n",
        "        self.model = joblib.load('model.pkl')\n",
        "\n",
        "    def pre_process_data(self):\n",
        "        ppr = PreprocessData(self.test_dataset)\n",
        "        ppr.clean_dataset()\n",
        "        return ppr.getDataset()\n",
        "\n",
        "    def prepare_result(self):\n",
        "        test_df = self.pre_process_data()\n",
        "        comments = test_df['original_text'].values\n",
        "        comments  = self.vectorizer.transform(comments)\n",
        "        res = self.model.predict(comments)\n",
        "        test_df['sentiments'] = res\n",
        "        test_df['sentiments']= test_df['sentiments'].apply(self.convert_pos_to_neg)\n",
        "        new_df = {\n",
        "            'id':test_df['id'].values,\n",
        "            'sentiment_class': test_df['sentiments'].values\n",
        "        }\n",
        "        new_df = pd.DataFrame(new_df)\n",
        "        return new_df\n",
        "\n",
        "    def getDataset(self):\n",
        "        return self.dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M09XZBYqdkD1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    data = PreprocessData(\"dataset/train.csv\")\n",
        "    data.clean_dataset()\n",
        "    data.save_preprocessed_data_set()\n",
        "    \n",
        "    smtAnly = SentimentModel(\"cleaned_comment.csv\",\"dataset/test.csv\")\n",
        "    smtAnly.vectorize_data()\n",
        "    smtAnly.train_model()\n",
        "    val = smtAnly.prepare_result()\n",
        "    val.to_csv('sub.csv',index = False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}